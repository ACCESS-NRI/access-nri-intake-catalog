# Copyright 2023 ACCESS-NRI and contributors. See the top-level COPYRIGHT file for details.
# SPDX-License-Identifier: Apache-2.0

import os
import re
import traceback

import cftime

import xarray as xr

from ecgtools.builder import Builder, INVALID_ASSET, TRACEBACK

# TO DO: There's probably (definitely) a better way to do this
CORE_PARSER_INFO = {
    "path": (
        lambda x: isinstance(x, str),
        "The entry 'path' generated by the parser must be string",
    ),
    "realm": (
        lambda x: isinstance(x, list) and all(isinstance(s, str) for s in x),
        "The entry 'realm' generated by the parser must be a list of strings",
    ),
    "variable": (
        lambda x: isinstance(x, list) and all(isinstance(s, str) for s in x),
        "The entry 'variable' generated by the parser must be a list of strings",
    ),
    "frequency": (
        lambda x: any(
            re.match(pattern, x)
            for pattern in [
                r"\d+_yearly",
                r"\d+_monthly",
                r"\d+_daily",
                r"\d+_hourly",
                "static",
            ]
        ),
        (
            "The entry 'frequency' generated by the parser must be one of 'N_yearly', "
            "'N_monthly', 'N_daily', 'N_hourly', 'static', where N is an integer"
        ),
    ),
    "start_date": (
        lambda x: True
        if re.match(r"\d\d\d\d-\d\d-\d\d,\s\d\d:\d\d:\d\d", x)
        else False,
        "The entry 'start_date' generated by the parser string with the format %Y-%m-%d, %H:%M:%S",
    ),
    "end_date": (
        lambda x: True
        if re.match(r"\d\d\d\d-\d\d-\d\d,\s\d\d:\d\d:\d\d", x)
        else False,
        "The entry 'end_date' generated by the parser string with the format %Y-%m-%d, %H:%M:%S",
    ),
}


class ParserValidationError(Exception):
    pass


class BaseParser(Builder):
    """
    Base class for creating intake-esm parsers. This is very similar to the ecgtools.Builder
    class, but it includes the parser as a staticmethod in the class, which makes validation
    of the parser output simpler
    """

    def _parse(self):
        super().parse(parsing_func=self.parser)

    def parse(self):
        self._parse()
        return self

    def validate_parser(self):
        """
        Run the parser on a single file and check the schema of the info being parsed
        """
        if not self.assets:
            raise ValueError(
                "asset list provided is None. Please run `.get_assets()` first"
            )

        invalid = True
        ind = 0
        while invalid:
            info = self.parser(self.assets[ind])
            if INVALID_ASSET in info:
                ind += 1
            else:
                invalid = False

        for key, (func, msg) in CORE_PARSER_INFO.items():
            if not func(info[key]):
                raise ParserValidationError(msg)

    @staticmethod
    def parser(file):
        """Override this: parse catalog information from a given file"""
        pass


class CosimaParser(BaseParser):
    """Parser for COSIMA datasets"""

    @staticmethod
    def parser(file):
        try:
            filename = os.path.basename(file)
            match_groups = re.match(
                r".*/([^/]*)/([^/]*)/output\d+/([^/]*)/.*\.nc", file
            ).groups()
            # configuration = match_groups[0]
            # experiment = match_groups[1]
            realm = match_groups[2]

            with xr.open_dataset(file, chunks={}, decode_times=False) as ds:
                variable_list = [var for var in ds if "long_name" in ds[var].attrs]

            info = {
                "path": str(file),
                "realm": [realm],
                "variable": variable_list,
                "filename": filename,
            }

            info["start_date"], info["end_date"], info["frequency"] = _get_timeinfo(ds)

            return info

        except Exception:
            return {INVALID_ASSET: file, TRACEBACK: traceback.format_exc()}


def _get_timeinfo(ds):
    """
    Stolen and slightly adapted from cosima cookbook
    https://github.com/COSIMA/cosima-cookbook/blob/master/cosima_cookbook/database.py#L565
    """
    time_dim = "time"  # TODO: this probably shouldn't be hardcoded
    if time_dim is None:
        return None

    time_var = ds[time_dim]
    has_bounds = hasattr(time_var, "bounds") and time_var.bounds in ds.variables

    def _todate(t):
        return cftime.num2date(t, time_var.units, calendar=time_var.calendar)

    if has_bounds:
        bounds_var = ds.variables[time_var.bounds]
        start_time = _todate(bounds_var[0, 0])
        end_time = _todate(bounds_var[-1, 1])
    else:
        start_time = _todate(time_var[0])
        end_time = _todate(time_var[-1])

    if len(time_var) > 1 or has_bounds:
        if has_bounds:
            next_time = _todate(bounds_var[0, 1])
        else:
            next_time = _todate(time_var[1])

        dt = next_time - start_time
        if dt.days >= 365:
            years = round(dt.days / 365)
            frequency = f"{years}_yearly"
        elif dt.days >= 28:
            months = round(dt.days / 30)
            frequency = f"{months}_monthly"
        elif dt.days >= 1:
            frequency = f"{dt.days}_daily"
        else:
            frequency = f"{dt.seconds // 3600}_hourly"
    else:
        # single time value in this file and no averaging
        frequency = "static"

    return (
        start_time.strftime("%Y-%m-%d, %H:%M:%S"),
        end_time.strftime("%Y-%m-%d, %H:%M:%S"),
        frequency,
    )
